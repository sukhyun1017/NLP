{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.NLP-텍스트 전처리",
      "provenance": [],
      "authorship_tag": "ABX9TyPh0Ws+qZHYHKId1nb+QG0q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukhyun1017/NLP-study/blob/main/1_NLP_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰화"
      ],
      "metadata": {
        "id": "XNhPExd-Dz3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RMtTfKsIo4I",
        "outputId": "81a4ce5d-4a5f-42fa-bb18-37c91920f371"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3hBzsIdEeYK",
        "outputId": "ce9d4999-bd62-4b4c-fa19-949f4b4e1676"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3Ey7p3_6Dw-O"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"Don't be fooled by the dark-based sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "metadata": {
        "id": "oxikc7DXECCd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 토큰화1 :',word_tokenize(sentence))\n",
        "print('단어 토큰화2 :',WordPunctTokenizer().tokenize(sentence))\n",
        "print('단어 토큰화3 :',text_to_word_sequence(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLCH0yTSD9ei",
        "outputId": "d7dcd28f-0be2-4baf-e6eb-41f4d9c46fa6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark-based', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', '-', 'based', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "단어 토큰화3 : [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'based', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#표준 토큰화\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndLaC7aaE5Fd",
        "outputId": "74615282-367a-4bef-fedd-974bc9c972a2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "트리뱅크 워드토크나이저 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark-based', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문장토큰화\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print('문장 토큰화1 :',sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRHB10acFjSK",
        "outputId": "811eb24a-1413-4016-d1f4-3acf272d65d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kss"
      ],
      "metadata": {
        "id": "6dS2TXA1GKeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#한국어 문장 토큰화\n",
        "import kss\n",
        "text = '딥러닝 자연어 처리 존잼 탱구리이다. 그러나 어쩔어렵이네. 츄라이잇해보삼'\n",
        "print('한국어 문장 토큰화: ',kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCEDqZ8yGVF_",
        "outputId": "1b5a9602-0d44-4ca8-9e9b-c2dc178758c2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 문장 토큰화:  ['딥러닝 자연어 처리 존잼 탱구리이다.', '그러나 어쩔어렵이네.', '츄라이잇해보삼']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "품사 태깅"
      ],
      "metadata": {
        "id": "2rXsd_PaIYmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "tokenized_sentence = word_tokenize(sentence)\n",
        "\n",
        "print('단어 토큰화 :',tokenized_sentence)\n",
        "print('품사 태깅 :',pos_tag(tokenized_sentence))\n",
        "#PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMTSff52GoEQ",
        "outputId": "519234a9-8dd4-4f34-e191-e07cc5587b53"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark-based', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "품사 태깅 : [('Do', 'VBP'), (\"n't\", 'RB'), ('be', 'VB'), ('fooled', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('dark-based', 'JJ'), ('sounding', 'NN'), ('name', 'NN'), (',', ','), ('Mr.', 'NNP'), ('Jone', 'NNP'), (\"'s\", 'POS'), ('Orphanage', 'NN'), ('is', 'VBZ'), ('as', 'RB'), ('cheery', 'JJ'), ('as', 'IN'), ('cheery', 'NN'), ('goes', 'VBZ'), ('for', 'IN'), ('a', 'DT'), ('pastry', 'NN'), ('shop', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "\n",
        "print('OKT 형태소 분석 :',okt.morphs(text))\n",
        "print('OKT 품사 태깅 :',okt.pos(text))\n",
        "print('OKT 명사 추출 :',okt.nouns(text)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3X4LsVlIEFa",
        "outputId": "74aa27fb-6b7e-41e3-ff80-c0ea94c5eea0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석 : ['딥', '러닝', '자연어', '처리', '존잼', '탱', '구리', '이다', '.', '그러나', '어쩔', '어렵이네', '.', '츄', '라이', '잇해', '보삼']\n",
            "OKT 품사 태깅 : [('딥', 'Noun'), ('러닝', 'Noun'), ('자연어', 'Noun'), ('처리', 'Noun'), ('존잼', 'Noun'), ('탱', 'Adverb'), ('구리', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation'), ('그러나', 'Conjunction'), ('어쩔', 'Adverb'), ('어렵이네', 'Adjective'), ('.', 'Punctuation'), ('츄', 'Noun'), ('라이', 'Noun'), ('잇해', 'Verb'), ('보삼', 'Noun')]\n",
            "OKT 명사 추출 : ['딥', '러닝', '자연어', '처리', '존잼', '구리', '츄', '라이', '보삼']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Kkma 형태소 분석:',kkma.morphs(text))\n",
        "print('Kkma 품사 태깅 :',kkma.pos(text))\n",
        "print('Kkma 명사 추출 :',kkma.nouns(text))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63p1DeDvIjg4",
        "outputId": "d0710a4d-5753-46da-8ab4-24d8714a1151"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kkma 형태소 분석: ['딥', '러닝', '자연어', '처리', '존', '잼', '탱', '구리', '이', '다', '.', '그러', '나', '어쩌', 'ㄹ', '어', '어', '렵', '이', '네', '.', '츄', '라이', '잇', '하', '어', '보삼']\n",
            "Kkma 품사 태깅 : [('딥', 'NNG'), ('러닝', 'NNG'), ('자연어', 'NNG'), ('처리', 'NNG'), ('존', 'NNP'), ('잼', 'NNG'), ('탱', 'UN'), ('구리', 'NNG'), ('이', 'VCP'), ('다', 'EFN'), ('.', 'SF'), ('그러', 'VV'), ('나', 'ECE'), ('어쩌', 'VV'), ('ㄹ', 'ETD'), ('어', 'VV'), ('어', 'ECS'), ('렵', 'UN'), ('이', 'VCP'), ('네', 'EFN'), ('.', 'SF'), ('츄', 'UN'), ('라이', 'NNG'), ('잇', 'NNG'), ('하', 'XSV'), ('어', 'ECS'), ('보삼', 'NNG')]\n",
            "Kkma 명사 추출 : ['딥', '딥러닝', '러닝', '자연어', '처리', '존', '존잼', '잼', '탱', '탱구리', '구리', '렵', '츄', '츄라이잇', '라이', '잇', '보삼']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "표제어추출"
      ],
      "metadata": {
        "id": "zRIxigHXLvD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "print('표제어 추출 전 :',words)\n",
        "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlVXU6I0Lx4i",
        "outputId": "05cc1e73-7843-4464-907a-628d74fe2ec9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어간추출"
      ],
      "metadata": {
        "id": "wjG_FpIfPOsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer() #포터 알고리즘\n",
        "lancaster_stemmer = LancasterStemmer() #랭커스터 스태머 알고리즘\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print('어간 추출 전 :', words)\n",
        "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
        "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7_3I5aZPLjE",
        "outputId": "5b3f0ad5-abe7-4d1e-c65a-7266609575a6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
            "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "불용어 (stopword)"
      ],
      "metadata": {
        "id": "uvknhXl6QAE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "UpSvqXZ8QbFg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_list = stopwords.words('english')\n",
        "print('불용어 개수 :', len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuFoPz9ZPYf7",
        "outputId": "7b33464f-d024-416e-ec8b-3cb029439b58"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for word in word_tokens: \n",
        "    if word not in stop_words: \n",
        "        result.append(word) \n",
        "\n",
        "print('불용어 제거 전 :',word_tokens) \n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xqA7yhfQjs_",
        "outputId": "ef185dd5-80d2-44bd-9dbd-a0f66fc1539d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한국어 불용어"
      ],
      "metadata": {
        "id": "wikKqXgJQ8Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words = set(stop_words.split(' '))\n",
        "word_tokens = okt.morphs(example)\n",
        "\n",
        "result = [word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print('불용어 제거 전 :',word_tokens) \n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LRXq0nOQ7mi",
        "outputId": "4476b26b-4594-461e-ff81-e04e93ba011d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정수인코딩"
      ],
      "metadata": {
        "id": "mjfnRw0SSsku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FreqDist\n",
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ufhY5aK1SuBx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "metadata": {
        "id": "TtIGq44TXKdn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj-ekpyQXMqn",
        "outputId": "a2d8beaa-4b89-4a44-95e8-6072a5e79009"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "    # 단어 토큰화\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "    result = []\n",
        "\n",
        "    for word in tokenized_sentence: \n",
        "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
        "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
        "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0 \n",
        "                vocab[word] += 1\n",
        "    preprocessed_sentences.append(result) \n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4gGtPilXNmH",
        "outputId": "c4a79aee-e1e5-4f7d-da6b-cdb20e0fd704"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = FreqDist(np.hstack(preprocessed_sentences))"
      ],
      "metadata": {
        "id": "tZ6IIkEbW-VW"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iggI2xuPXyrU",
        "outputId": "529f5745-b302-4d4d-a66b-6f03e33a3361"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVIQLogIXzld",
        "outputId": "8ac9ab1b-ee1e-429e-cf9b-153c8cd7e6a7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "DcGuwIQZX8Ln"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
        "tokenizer.fit_on_texts(preprocessed_sentences) \n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjcaQYvRYBmk",
        "outputId": "5c454da9-1209-470a-b8a8-0a02d4279bef"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자 0(padding)과 OOV(out of vocabulary)를 고려해서 단어 집합의 크기는 +2\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV') #자동으로 빈도순으로 뽑아줌\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences)) #text_to_sequence 코퍼스를 인덱스로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ7Rzq4rYbBg",
        "outputId": "e8755fbc-8614-4777-9473-495d5f8e1c9d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 OOV의 인덱스 : 1\n",
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "padding"
      ],
      "metadata": {
        "id": "gheZi1S5aV7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded=tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "max_len = max(len(item) for item in encoded)\n",
        "print('최대 길이 :',max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-jkKfjhZONw",
        "outputId": "3741cc19-fa32-453c-d002-64971f53005c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 : 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in encoded:\n",
        "    while len(sentence) < max_len:\n",
        "        sentence.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwUqOrKFaiSr",
        "outputId": "7c05ef92-4669-4e10-c8c6-f39007d46bfc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 6, 0, 0, 0, 0, 0],\n",
              "       [2, 1, 6, 0, 0, 0, 0],\n",
              "       [2, 4, 6, 0, 0, 0, 0],\n",
              "       [1, 3, 0, 0, 0, 0, 0],\n",
              "       [3, 5, 4, 3, 0, 0, 0],\n",
              "       [4, 3, 0, 0, 0, 0, 0],\n",
              "       [2, 5, 1, 0, 0, 0, 0],\n",
              "       [2, 5, 1, 0, 0, 0, 0],\n",
              "       [2, 5, 3, 0, 0, 0, 0],\n",
              "       [1, 1, 4, 3, 1, 2, 1],\n",
              "       [2, 1, 4, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#케라스로 패딩\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "GJXrzCgbgbTd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw7ztAVEgjyp",
        "outputId": "7b32ab3f-2342-44b0-8492-a69cd1a58646"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded,padding='post')\n",
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG3H-e0rgqFA",
        "outputId": "a8187a0a-39e1-4606-bd57-ce778cfe182c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 6, 0, 0, 0, 0, 0],\n",
              "       [2, 1, 6, 0, 0, 0, 0],\n",
              "       [2, 4, 6, 0, 0, 0, 0],\n",
              "       [1, 3, 0, 0, 0, 0, 0],\n",
              "       [3, 5, 4, 3, 0, 0, 0],\n",
              "       [4, 3, 0, 0, 0, 0, 0],\n",
              "       [2, 5, 1, 0, 0, 0, 0],\n",
              "       [2, 5, 1, 0, 0, 0, 0],\n",
              "       [2, 5, 3, 0, 0, 0, 0],\n",
              "       [1, 1, 4, 3, 1, 2, 1],\n",
              "       [2, 1, 4, 1, 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded,padding='post',maxlen=5)\n",
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2cSNTpig7Zb",
        "outputId": "daf8d43c-9625-48db-d747-5fa6274b819e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 6, 0, 0, 0],\n",
              "       [2, 1, 6, 0, 0],\n",
              "       [2, 4, 6, 0, 0],\n",
              "       [1, 3, 0, 0, 0],\n",
              "       [3, 5, 4, 3, 0],\n",
              "       [4, 3, 0, 0, 0],\n",
              "       [2, 5, 1, 0, 0],\n",
              "       [2, 5, 1, 0, 0],\n",
              "       [2, 5, 3, 0, 0],\n",
              "       [4, 3, 1, 2, 1],\n",
              "       [2, 1, 4, 1, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded, padding='post', truncating='post', maxlen=5) #truncating 만약 데이터가 손실된다면 뒤의 단어 제거\n",
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rbOQrkmhGvL",
        "outputId": "2bf9a564-ce03-4409-b8be-11914db1d199"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 6, 0, 0, 0],\n",
              "       [2, 1, 6, 0, 0],\n",
              "       [2, 4, 6, 0, 0],\n",
              "       [1, 3, 0, 0, 0],\n",
              "       [3, 5, 4, 3, 0],\n",
              "       [4, 3, 0, 0, 0],\n",
              "       [2, 5, 1, 0, 0],\n",
              "       [2, 5, 1, 0, 0],\n",
              "       [2, 5, 3, 0, 0],\n",
              "       [1, 1, 4, 3, 1],\n",
              "       [2, 1, 4, 1, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "원핫인코딩"
      ],
      "metadata": {
        "id": "vZT2tcgPiJax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "yzCWi845j4hZ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWdasrxniMKv",
        "outputId": "b4fb626e-22c5-4e3f-c5ae-b767d61a8441"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'OOV': 1,\n",
              " 'barber': 2,\n",
              " 'crazy': 12,\n",
              " 'driving': 11,\n",
              " 'good': 9,\n",
              " 'huge': 4,\n",
              " 'keeping': 8,\n",
              " 'kept': 5,\n",
              " 'knew': 10,\n",
              " 'mountain': 14,\n",
              " 'person': 6,\n",
              " 'secret': 3,\n",
              " 'went': 13,\n",
              " 'word': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded=tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgV1fjbfijrX",
        "outputId": "52fde669-40d8-4840-d9e5-063ce8e71693"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot=to_categorical(encoded[1])\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaQbGjwijamZ",
        "outputId": "73da0c6e-6cc7-4a2c-9e11-8b5e4c151468"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eonVb4KPknlH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}