{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.NLP-다른 임베딩방법들",
      "provenance": [],
      "authorship_tag": "ABX9TyP18PiMrUr8Bzd5y4LFp2YR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukhyun1017/NLP-study/blob/main/5_NLP_%EB%8B%A4%EB%A5%B8_%EC%9E%84%EB%B2%A0%EB%94%A9%EB%B0%A9%EB%B2%95%EB%93%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glove"
      ],
      "metadata": {
        "id": "UgE4x_jaNEgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 카운트 기반과 예측 기반을 모두 사용하는 임베딩 방법론"
      ],
      "metadata": {
        "id": "KVcx3OTHNHBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfthWnmfI_U3",
        "outputId": "be6ced20-d8d4-410f-c894-3e2a531256b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoKL5XrH_3ZW",
        "outputId": "9b388b77-06ef-4cfe-9f53-5b9070a10941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting glove_python_binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 102 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 194 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 204 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 215 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 235 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 286 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 307 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 317 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 389 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 399 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 409 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 419 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 430 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 440 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 481 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 491 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 501 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 512 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 522 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 532 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 542 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 552 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 573 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 583 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 604 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 614 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 634 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 645 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 665 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 675 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 686 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 696 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 706 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 716 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 727 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 737 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 747 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 757 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 768 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 778 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 788 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 798 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 808 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 819 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 829 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 839 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 849 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 860 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 870 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 880 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 901 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 911 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 931 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 942 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 948 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install glove_python_binary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "p797BYccIsjr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts1gM96jIt4u",
        "outputId": "e7bab2bb-3760-4c07-ac44-b33979ef3765"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7f9fc7016bd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
        "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "metadata": {
        "id": "BMHhh8NoIvA8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"
      ],
      "metadata": {
        "id": "mLifeVnDRMsh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus() \n",
        "\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
        "corpus.fit(result, window=5) #주변 5개의 단어\n",
        "glove = Glove(no_components=100, learning_rate=0.05) #no_components 아웃풋벡터 차원\n",
        "\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True) #쓰레드? 프로세스가 할당받은 자원을 실행하는 단위\n",
        "# 유사도 검색을 위한 행렬의 index 정보 입력\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUP_v8RRIaJO",
        "outputId": "122c1bd7-5808-40d3-e81a-154d3b944c7e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"man\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8hCbEE7KdGS",
        "outputId": "f7ddda28-4534-4c31-9c5c-bcf62cf2920c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.9576905413081727), ('girl', 0.885377350881285), ('guy', 0.8791334184021801), ('boy', 0.844628136898159)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"university\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dkhtRO_Kgih",
        "outputId": "643ec092-2834-4298-e81d-2d0749559011"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('harvard', 0.891247581648307), ('mit', 0.8675963250567178), ('stanford', 0.845807684940681), ('cambridge', 0.8436831310992694)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText"
      ],
      "metadata": {
        "id": "7woiy2M9NYRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText 는 내부 단어를 학습합\n",
        "# n = 3인 경우 <ap, app, ppl, ple, le> ,<apple> 를 벡터값을 구해서 다 더함"
      ],
      "metadata": {
        "id": "xdiTksWuT_Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존의 word2vec은 없는 단어에 대해 error 를 일으킴\n",
        "model.wv.most_similar(\"electrofishing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "WB7OeCRGN744",
        "outputId": "005a5c5d-6c59-4b0b-8ef8-f4ad15a9e6ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f9544fe3d5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'electrofishing' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)\n",
        "#size = 벡터의 차원\n",
        "#worker = number of threads "
      ],
      "metadata": {
        "id": "yWjedHT8Tk9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FastText 는 없는 단어도 찾아냄\n",
        "model.wv.most_similar(\"electrofishing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4HojVBKS7iR",
        "outputId": "8f4c9d2f-ca02-4e5e-d319-82d1b989581d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('electrolux', 0.8010354042053223),\n",
              " ('electric', 0.7995267510414124),\n",
              " ('electro', 0.7939893007278442),\n",
              " ('electrolyte', 0.7906008958816528),\n",
              " ('electrochemical', 0.7675807476043701),\n",
              " ('electroshock', 0.7577415704727173),\n",
              " ('gastric', 0.7560892105102539),\n",
              " ('electrons', 0.7499829530715942),\n",
              " ('plutonium', 0.7465008497238159),\n",
              " ('electrified', 0.7434133291244507)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Train"
      ],
      "metadata": {
        "id": "clZbA5B8T3J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "vocab_size = 20000\n",
        "output_dim = 128\n",
        "input_length = 500\n",
        "\n",
        "v = Embedding(vocab_size, output_dim, input_length=input_length)"
      ],
      "metadata": {
        "id": "hiU-SJPWTl05"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1] #긍정 =1 / 부정 = 0"
      ],
      "metadata": {
        "id": "gHri9Dfta9rP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) \n",
        "vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1\n",
        "print('단어 집합 :',vocab_size)\n",
        "print(tokenizer.word_index)\n",
        "#it_on_texts와 word_index를 사용하여 key value로 이루어진 딕셔너리를 만든다. \n",
        "#그 후에 texts_to_sequences를 이용하여 text 문장을 숫자로 이루어진 리스트로 만든다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCBXksqsbD86",
        "outputId": "984f0511-9311-45e5-8425-8e81f793d140"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : 16\n",
            "{'nice': 1, 'great': 2, 'best': 3, 'amazing': 4, 'stop': 5, 'lies': 6, 'pitiful': 7, 'nerd': 8, 'excellent': 9, 'work': 10, 'supreme': 11, 'quality': 12, 'bad': 13, 'highly': 14, 'respectable': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print('정수 인코딩 결과 :',X_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvUtJYWnblW7",
        "outputId": "ecd1fab1-3c11-418e-f4e9-0e73d2a02c1b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 결과 : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in X_encoded)\n",
        "print('최대 길이 :',max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl5ZglCFbnJn",
        "outputId": "e808bb14-8a13-4c06-d7fc-ef0f2bdaf7b8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print('패딩 결과 :')\n",
        "print(X_train)\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujVv3IX0em0s",
        "outputId": "560f993a-2ec1-415b-c0a6-f11a36160eb0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "패딩 결과 :\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n",
            "[1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Trained glove"
      ],
      "metadata": {
        "id": "cXpn326FgVbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "umvRDe6PfA3E"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "\n",
        "    # 100개의 값을 가지는 array로 변환\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # asarray:데이터 형태가 다를 경우에만 복사(copy) 가 된다.\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkVX_NsKfqQ0",
        "outputId": "f19eb0e3-978b-4479-d309-d5f013707898"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000개의 Embedding vector가 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['respectable'])\n",
        "print('벡터의 차원 수 :',len(embedding_dict['respectable']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryK7zsQKggJu",
        "outputId": "04704305-f4a2-4821-ac6e-dd346035425a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
            "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
            "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
            "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
            " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
            " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
            " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
            " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
            " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
            " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
            " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
            "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
            "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
            "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
            " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
            " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
            " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
            "벡터의 차원 수 : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EepdDS9xgitA",
        "outputId": "8bad39dd-e121-4b2a-f020-51353dc3b524"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 행렬의 크기(shape) : (16, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYTdQixugyij",
        "outputId": "e0577fc1-7a26-4fa2-a31e-3f9f13a33f0e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 great의 맵핑된 정수 :',tokenizer.word_index['great'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEZT7R9kg4kP",
        "outputId": "401c8872-cdef-4e5d-b4f9-122a1d087230"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 great의 맵핑된 정수 : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['great'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDNO6aVqiB_R",
        "outputId": "ecc63285-70e2-40ab-903c-c1c01a2377d7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
            " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
            " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
            " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
            "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
            " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
            " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
            "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
            "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
            "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
            " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
            "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
            "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
            " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
            " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
            " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
            " -0.69183   -1.0426     0.28855    0.63056  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = embedding_dict.get(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "metadata": {
        "id": "4w-NFPL9g7BI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[2]==embedding_dict['great']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDJk3EkSh5T4",
        "outputId": "929364de-1fba-4c61-cc5d-713fd6218af3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedding_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBFrN7umicz8",
        "outputId": "d0375f2a-aad1-4158-aa35-49adb11f87e8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "output_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=5, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uypZl40zh7u-",
        "outputId": "a410f5d5-4e8b-4d29-c8c3-3b28434d4212"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1/1 - 1s - loss: 0.7632 - acc: 0.2857 - 1s/epoch - 1s/step\n",
            "Epoch 2/5\n",
            "1/1 - 0s - loss: 0.7403 - acc: 0.2857 - 9ms/epoch - 9ms/step\n",
            "Epoch 3/5\n",
            "1/1 - 0s - loss: 0.7183 - acc: 0.4286 - 19ms/epoch - 19ms/step\n",
            "Epoch 4/5\n",
            "1/1 - 0s - loss: 0.6971 - acc: 0.4286 - 15ms/epoch - 15ms/step\n",
            "Epoch 5/5\n",
            "1/1 - 0s - loss: 0.6768 - acc: 0.7143 - 17ms/epoch - 17ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f2ef9c190>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Trained Wrod2vec"
      ],
      "metadata": {
        "id": "dZRzliQ-lOZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
        "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "print('모델의 크기(shape) :',word2vec_model.vectors.shape) # 모델의 크기 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUu4WHFdj17D",
        "outputId": "17977135-9d94-40fd-8da9-5e369dc89f2d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델의 크기(shape) : (3000000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I1E8gEtl_yA",
        "outputId": "700dbdd4-3ce2-4d75-a104-ae309e330b75"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 행렬의 크기(shape) : (16, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector(word):\n",
        "    if word in word2vec_model:\n",
        "        return word2vec_model[word]\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "j6Mgts-7mFvr"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#100차원 임베딩 벡터 'great'가 존재합니다\n",
        "word2vec_model['great']"
      ],
      "metadata": {
        "id": "cpOERX6cmjSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = get_vector(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "metadata": {
        "id": "oP4kdLetmWdi"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 nice의 맵핑된 정수 :', tokenizer.word_index['nice'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERDpVAcHnLX0",
        "outputId": "fbccb44b-5beb-4309-f66d-772f2782cb17"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 nice의 맵핑된 정수 : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix[1]==word2vec_model['nice'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1XeCkkwnTfN",
        "outputId": "d7263d5c-a88f-42f4-a9ff-ac87bbe37cf6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpdOQQFYnaRs",
        "outputId": "f56d8674-09b9-40e6-c979-77c9444fe94f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.6785 - acc: 0.2857 - 1s/epoch - 1s/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6599 - acc: 0.7143 - 10ms/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6419 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6244 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6075 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.5912 - acc: 1.0000 - 22ms/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.5754 - acc: 1.0000 - 21ms/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.5602 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.5455 - acc: 1.0000 - 24ms/epoch - 24ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.5313 - acc: 1.0000 - 22ms/epoch - 22ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.5177 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.5045 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.4918 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.4796 - acc: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.4677 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.4564 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.4454 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.4348 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.4245 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.4146 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.4051 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.3958 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.3869 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.3782 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.3699 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.3618 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.3539 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.3463 - acc: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.3389 - acc: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.3317 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.3248 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.3181 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.3115 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.3052 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.2990 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.2930 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.2872 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.2816 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.2761 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.2707 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.2656 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.2605 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.2556 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.2509 - acc: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.2462 - acc: 1.0000 - 29ms/epoch - 29ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.2417 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.2373 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.2331 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.2289 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.2249 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.2209 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.2171 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.2133 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.2097 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.2061 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2027 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.1993 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.1960 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.1928 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.1897 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.1866 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.1837 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.1808 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.1779 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.1752 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.1725 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.1698 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.1673 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.1647 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.1623 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.1599 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.1575 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.1552 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.1530 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.1508 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.1487 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.1466 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.1445 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.1425 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.1405 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.1386 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.1367 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.1349 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.1331 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.1313 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.1296 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.1279 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.1263 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.1246 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.1230 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.1215 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.1199 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.1184 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.1170 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.1155 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.1141 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.1127 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.1114 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.1100 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.1087 - acc: 1.0000 - 6ms/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f23e06d90>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MMEQ8S-mnwYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}